{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47d7a34",
   "metadata": {},
   "source": [
    "Limpieza de datos y generacion de n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import os  \n",
    "import re  \n",
    "from collections import Counter  \n",
    "from nltk.util import ngrams  # Para generar n-gramas (pares o grupos de palabras consecutivas)\n",
    "\n",
    "# Ruta donde se encuentran los documentos generados\n",
    "directorio_documentos = '../documentos/documentos_generados'\n",
    "\n",
    "# Función para limpiar y tokenizar texto\n",
    "def limpiar_y_tokenizar(texto):\n",
    "    \"\"\"\n",
    "    Esta función toma un texto como entrada y realiza los siguientes pasos:\n",
    "    1. Convierte todo el texto a minúsculas para evitar diferencias entre palabras como \"Hola\" y \"hola\".\n",
    "    2. Elimina signos de puntuación utilizando regex.\n",
    "    3. Divide el texto en palabras individuales (tokens) separadas por espacios.\n",
    "    \"\"\"\n",
    "    texto = texto.lower()\n",
    "    # Eliminar signos de puntuación (todo lo que no sea letras, números o espacios)\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
    "    # Dividir el texto en palabras (tokens)\n",
    "    tokens = texto.split()\n",
    "    return tokens  # Devolvemos la lista de palabras\n",
    "\n",
    "# Función para generar n-gramas\n",
    "def generar_ngrams(tokens, n=2):\n",
    "    \"\"\"\n",
    "    Esta función genera n-gramas a partir de una lista de palabras (tokens).\n",
    "    Un n-grama es una secuencia de 'n' palabras consecutivas.\n",
    "    Por ejemplo:\n",
    "    - Para n=2 (bi-gramas): ['hola', 'mundo'] -> [('hola', 'mundo')]\n",
    "    \"\"\"\n",
    "    return list(ngrams(tokens, n))  # Usamos la función ngrams de nltk para generar los n-gramas\n",
    "\n",
    "# Procesar todos los documentos de texto\n",
    "for archivo in os.listdir(directorio_documentos):  \n",
    "    # Construimos la ruta completa del archivo\n",
    "    ruta_archivo = os.path.join(directorio_documentos, archivo) #agregamos el nombre del archivo a la ruta\n",
    "    \n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "        texto = f.read()\n",
    "        \n",
    "        # Limpieza y tokenización del texto\n",
    "        tokens = limpiar_y_tokenizar(texto)\n",
    "        \n",
    "        # Generar bi-gramas (pares de palabras consecutivas)\n",
    "        bigramas = generar_ngrams(tokens, 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb5126",
   "metadata": {},
   "source": [
    "Creacion de tabla hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2627ab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Hash de n-gramas:\n",
      "('codigo', 'algoritmo'): 20\n",
      "('algoritmo', 'documento'): 26\n",
      "('documento', 'texto'): 22\n",
      "('texto', 'codigo'): 21\n",
      "('codigo', 'mundo'): 13\n",
      "('mundo', 'mundo'): 13\n",
      "('mundo', 'python'): 18\n",
      "('python', 'hola'): 28\n",
      "('hola', 'mundo'): 22\n",
      "('python', 'inteligente'): 23\n",
      "('inteligente', 'documento'): 24\n",
      "('documento', 'inteligente'): 27\n",
      "('inteligente', 'inteligente'): 23\n",
      "('documento', 'aprendizaje'): 20\n",
      "('aprendizaje', 'inteligente'): 17\n",
      "('documento', 'programar'): 25\n",
      "('programar', 'hola'): 21\n",
      "('hola', 'python'): 29\n",
      "('hola', 'programar'): 19\n",
      "('hola', 'hola'): 26\n",
      "('hola', 'texto'): 22\n",
      "('texto', 'python'): 17\n",
      "('python', 'codigo'): 22\n",
      "('algoritmo', 'codigo'): 20\n",
      "('codigo', 'inteligente'): 16\n",
      "('inteligente', 'python'): 22\n",
      "('python', 'programar'): 10\n",
      "('programar', 'inteligente'): 26\n",
      "('inteligente', 'aprendizaje'): 21\n",
      "('aprendizaje', 'python'): 23\n",
      "('inteligente', 'codigo'): 22\n",
      "('inteligente', 'algoritmo'): 17\n",
      "('codigo', 'texto'): 17\n",
      "('texto', 'texto'): 14\n",
      "('texto', 'documento'): 19\n",
      "('inteligente', 'hola'): 26\n",
      "('python', 'texto'): 20\n",
      "('texto', 'hola'): 16\n",
      "('codigo', 'programar'): 25\n",
      "('programar', 'texto'): 16\n",
      "('hola', 'inteligente'): 16\n",
      "('aprendizaje', 'hola'): 21\n",
      "('codigo', 'python'): 17\n",
      "('texto', 'inteligente'): 20\n",
      "('algoritmo', 'programar'): 23\n",
      "('programar', 'codigo'): 21\n",
      "('mundo', 'programar'): 23\n",
      "('programar', 'algoritmo'): 24\n",
      "('algoritmo', 'algoritmo'): 19\n",
      "('documento', 'documento'): 14\n",
      "('texto', 'algoritmo'): 15\n",
      "('aprendizaje', 'programar'): 18\n",
      "('codigo', 'aprendizaje'): 22\n",
      "('aprendizaje', 'aprendizaje'): 16\n",
      "('aprendizaje', 'documento'): 20\n",
      "('algoritmo', 'mundo'): 16\n",
      "('mundo', 'algoritmo'): 21\n",
      "('programar', 'programar'): 18\n",
      "('programar', 'mundo'): 22\n",
      "('mundo', 'aprendizaje'): 21\n",
      "('hola', 'documento'): 24\n",
      "('documento', 'codigo'): 16\n",
      "('inteligente', 'texto'): 19\n",
      "('codigo', 'documento'): 26\n",
      "('documento', 'hola'): 24\n",
      "('hola', 'aprendizaje'): 17\n",
      "('texto', 'programar'): 24\n",
      "('inteligente', 'mundo'): 21\n",
      "('mundo', 'hola'): 18\n",
      "('python', 'mundo'): 18\n",
      "('mundo', 'texto'): 15\n",
      "('texto', 'mundo'): 18\n",
      "('mundo', 'codigo'): 18\n",
      "('mundo', 'inteligente'): 21\n",
      "('programar', 'python'): 18\n",
      "('hola', 'codigo'): 22\n",
      "('python', 'algoritmo'): 25\n",
      "('algoritmo', 'texto'): 19\n",
      "('python', 'documento'): 25\n",
      "('documento', 'mundo'): 16\n",
      "('mundo', 'documento'): 15\n",
      "('python', 'aprendizaje'): 15\n",
      "('aprendizaje', 'mundo'): 23\n",
      "('aprendizaje', 'algoritmo'): 17\n",
      "('algoritmo', 'aprendizaje'): 15\n",
      "('codigo', 'hola'): 17\n",
      "('codigo', 'codigo'): 16\n",
      "('inteligente', 'programar'): 16\n",
      "('programar', 'aprendizaje'): 16\n",
      "('programar', 'documento'): 17\n",
      "('documento', 'python'): 23\n",
      "('documento', 'algoritmo'): 20\n",
      "('texto', 'aprendizaje'): 21\n",
      "('hola', 'algoritmo'): 20\n",
      "('algoritmo', 'inteligente'): 22\n",
      "('algoritmo', 'python'): 21\n",
      "('aprendizaje', 'texto'): 20\n",
      "('algoritmo', 'hola'): 22\n",
      "('aprendizaje', 'codigo'): 9\n",
      "('python', 'python'): 16\n"
     ]
    }
   ],
   "source": [
    "# Crear una tabla hash para almacenar los n-gramas\n",
    "tabla_hash = {}\n",
    "\n",
    "# Función para agregar n-gramas a la tabla hash\n",
    "def agregar_a_tabla_hash(ngrams, tabla_hash):\n",
    "    for ngram in ngrams:\n",
    "        if ngram in tabla_hash:\n",
    "            tabla_hash[ngram] += 1\n",
    "        else:\n",
    "            tabla_hash[ngram] = 1\n",
    "\n",
    "# Procesar todos los documentos y almacenar los n-gramas en la tabla hash\n",
    "for archivo in os.listdir(directorio_documentos):\n",
    "    ruta_archivo = os.path.join(directorio_documentos, archivo)\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "        texto = f.read()\n",
    "        # Limpieza y tokenización\n",
    "        tokens = limpiar_y_tokenizar(texto)\n",
    "        # Generar bi-gramas y tri-gramas\n",
    "        bigramas = generar_ngrams(tokens, 2)\n",
    "        # Agregar n-gramas a la tabla hash\n",
    "        agregar_a_tabla_hash(bigramas, tabla_hash)\n",
    "\n",
    "# Mostrar los n-gramas almacenados en la tabla hash\n",
    "print(\"Tabla Hash de n-gramas:\")\n",
    "for ngram, frecuencia in tabla_hash.items():\n",
    "    print(f\"{ngram}: {frecuencia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b54d2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la Similitud de Jaccard\n",
    "def calcular_similitud_jaccard(ngrams_doc1, ngrams_doc2):\n",
    "    # Convertir los n-gramas a conjuntos\n",
    "    conjunto1 = set(ngrams_doc1)\n",
    "    conjunto2 = set(ngrams_doc2)\n",
    "    # Calcular la intersección y la unión\n",
    "    interseccion = len(conjunto1.intersection(conjunto2))\n",
    "    union = len(conjunto1.union(conjunto2))\n",
    "    # Retornar la similitud de Jaccard\n",
    "    return interseccion / union if union != 0 else 0\n",
    "\n",
    "# Comparar documentos en el directorio y almacenar similitudes\n",
    "documentos = os.listdir(directorio_documentos)\n",
    "similitudes = []\n",
    "\n",
    "for i in range(len(documentos)):\n",
    "    for j in range(i + 1, len(documentos)):\n",
    "        # Leer y procesar el primer documento\n",
    "        ruta_doc1 = os.path.join(directorio_documentos, documentos[i])\n",
    "        with open(ruta_doc1, 'r', encoding='utf-8') as f1:\n",
    "            texto1 = f1.read()\n",
    "            tokens1 = limpiar_y_tokenizar(texto1)\n",
    "            ngrams_doc1 = generar_ngrams(tokens1, 2)  # Cambiar a 3 para tri-gramas\n",
    "\n",
    "        # Leer y procesar el segundo documento\n",
    "        ruta_doc2 = os.path.join(directorio_documentos, documentos[j])\n",
    "        with open(ruta_doc2, 'r', encoding='utf-8') as f2:\n",
    "            texto2 = f2.read()\n",
    "            tokens2 = limpiar_y_tokenizar(texto2)\n",
    "            ngrams_doc2 = generar_ngrams(tokens2, 2)  # Cambiar a 3 para tri-gramas\n",
    "\n",
    "        # Calcular la similitud de Jaccard\n",
    "        similitud = calcular_similitud_jaccard(ngrams_doc1, ngrams_doc2)\n",
    "        similitudes.append((documentos[i], documentos[j], similitud))\n",
    "\n",
    "# Ordenar las similitudes de mayor a menor\n",
    "similitudes.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d604d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documentos más similares:\n",
      "documento_10.txt y documento_9.txt: 0.62\n",
      "documento_19.txt y documento_3.txt: 0.59\n",
      "documento_3.txt y documento_4.txt: 0.59\n",
      "documento_10.txt y documento_19.txt: 0.59\n",
      "documento_4.txt y documento_9.txt: 0.59\n"
     ]
    }
   ],
   "source": [
    "# Algoritmo Merge Sort para ordenar las similitudes\n",
    "def merge_sort(similitudes):\n",
    "    if len(similitudes) <= 1:\n",
    "        return similitudes\n",
    "\n",
    "    # Dividir la lista en dos mitades\n",
    "    mid = len(similitudes) // 2\n",
    "    izquierda = merge_sort(similitudes[:mid])\n",
    "    derecha = merge_sort(similitudes[mid:])\n",
    "\n",
    "    # Combinar las mitades ordenadas\n",
    "    return merge(izquierda, derecha)\n",
    "\n",
    "def merge(izquierda, derecha):\n",
    "    resultado = []\n",
    "    i = j = 0\n",
    "\n",
    "    # Comparar y combinar las dos listas\n",
    "    while i < len(izquierda) and j < len(derecha):\n",
    "        if izquierda[i][2] > derecha[j][2]:  # Ordenar por similitud (índice 2)\n",
    "            resultado.append(izquierda[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            resultado.append(derecha[j])\n",
    "            j += 1\n",
    "\n",
    "    # Agregar los elementos restantes\n",
    "    resultado.extend(izquierda[i:])\n",
    "    resultado.extend(derecha[j:])\n",
    "    return resultado\n",
    "\n",
    "# Ordenar las similitudes usando Merge Sort\n",
    "similitudes_ordenadas = merge_sort(similitudes)\n",
    "\n",
    "# Mostrar los N documentos más similares\n",
    "N = 5  # Cambia este valor para mostrar más o menos resultados\n",
    "print(f\"Top {N} documentos más similares:\")\n",
    "for doc1, doc2, similitud in similitudes_ordenadas[:N]:\n",
    "    print(f\"{doc1} y {doc2}: {similitud:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
